# ğŸŒ First Data Project - Weather ETL Pipeline

> **A complete ETL pipeline to extract, transform and load weather data from 5 cities around the world into a MySQL database**

---

## ğŸ“‹ Table of Contents

- [ğŸ¯ Overview](#-overview)
- [âœ¨ Features](#-features)
- [ğŸ—ï¸ Architecture](#-architecture)
- [ğŸ“¦ Installation](#-installation)
- [ğŸš€ Usage](#-usage)
- [ğŸ“‚ Project Structure](#-project-structure)
- [ğŸ”„ ETL Pipeline](#-etl-pipeline)
- [ğŸ› ï¸ Configuration](#-configuration)
- [ğŸ“Š Data](#-data)
- [ğŸ› Troubleshooting](#-troubleshooting)
- [ğŸ“ License](#-license)

---

## ğŸ¯ Overview

This project implements an automated **ETL (Extract, Transform, Load) pipeline** to:

âœ… **Extract** real-time weather data from the OpenWeatherMap API
âœ… **Transform** raw data to clean and normalize it
âœ… **Load** transformed data into a MySQL database

The pipeline handles errors gracefully, saves data locally in CSV/JSON format, and includes a comprehensive logging system.

---

## âœ¨ Features

| Feature | Description |
|---|---|
| ğŸŒ **API Extraction** | Fetches weather data from OpenWeatherMap for 5 major cities |
| ğŸ§¹ **Data Cleaning** | Removes duplicates, handles missing values |
| ğŸ“Š **Transformation** | Normalizes temperature units, structures data |
| ğŸ’¾ **Multi-format Storage** | Saves in CSV, JSON and MySQL |
| ğŸ“ **Detailed Logging** | Records all pipeline steps |
| âš¡ **Error Handling** | Graceful fallback if database is unavailable |
| ğŸ” **Secure Configuration** | Credentials stored in environment variables |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MAIN.PY - Orchestrator                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  EXTRACT      â”‚            â”‚   TRANSFORM       â”‚
    â”‚  - API Call   â”‚            â”‚   - Cleaning      â”‚
    â”‚  - JSON Save  â”‚            â”‚   - Normalization â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                             â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    LOAD      â”‚
                    â”‚  - MySQL DB  â”‚
                    â”‚  - CSV/JSON  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Installation

### ğŸ“‹ Prerequisites

- **Python** 3.8+
- **MySQL Server** 5.7+ (optional for local backup)
- **pip** (Python package manager)

### ğŸ”§ Step 1: Clone the repository

```bash
git clone <your-repo>
cd First_Data_Project
```

### ğŸ”§ Step 2: Create a virtual environment

```bash
# On Windows
python -m venv venv
venv\Scripts\activate

# On macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### ğŸ”§ Step 3: Install dependencies

```bash
pip install -r requirements.txt
```

### ğŸ”§ Step 4: Set up environment variables

Create a `.env` file at the project root (optional):

```env
DB_USER=davy
DB_PASSWORD=password123
DB_HOST=localhost
DB_NAME=First_Data
API_KEY=21df4d73e5dc83ea09d6f0ed3148d2bc
```

### ğŸ”§ Step 5: Configure MySQL (optional)

```bash
# Check MySQL status
sudo systemctl status mysql

# Start MySQL if needed
sudo systemctl start mysql

# Create the database
mysql -u root -p
> CREATE DATABASE First_Data;
```

---

## ğŸš€ Usage

### â–¶ï¸ Run the complete pipeline

```bash
python main.py
```

**Expected output:**
```
2026-02-04 23:31:25,222 - INFO - Logger configured.
[EXTRACT] Starting extraction...
2026-02-04 23:32:14,137 - INFO - Data for Ouagadougou: {...}
...
2026-02-04 23:32:15,357 - INFO - [TRANSFORM] Cleaned data saved to clean.csv
2026-02-04 23:32:15,662 - INFO - Data loaded successfully
```

### â–¶ï¸ Run extraction only

```bash
python -m etl.Extract
```

### â–¶ï¸ Run transformation only

```bash
python -m etl.Transform
```

### â–¶ï¸ Run loading only

```bash
python -m etl.Load
```

---

## ğŸ“‚ Project Structure

```
First_Data_Project/
â”‚
â”œâ”€â”€ ğŸ“„ main.py                 # Main entry point
â”œâ”€â”€ ğŸ“„ requirements.txt         # Project dependencies
â”œâ”€â”€ ğŸ“„ README.md               # French documentation
â”œâ”€â”€ ğŸ“„ README_EN.md            # English documentation
â”‚
â”œâ”€â”€ ğŸ“ config/                 # Configuration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ logger.py              # Logging configuration
â”‚
â”œâ”€â”€ ğŸ“ etl/                    # ETL Pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ Extract.py             # Extraction step (API)
â”‚   â”œâ”€â”€ Transform.py           # Transformation step (cleaning)
â”‚   â””â”€â”€ Load.py                # Loading step (Database)
â”‚
â”œâ”€â”€ ğŸ“ data/                   # Raw data (JSON)
â”‚   â”œâ”€â”€ Ouagadougou.json
â”‚   â”œâ”€â”€ New York.json
â”‚   â”œâ”€â”€ Londres.json
â”‚   â”œâ”€â”€ Tokyo.json
â”‚   â””â”€â”€ Sydney.json
â”‚
â”œâ”€â”€ ğŸ“ data_clean/             # Transformed data
â”‚   â”œâ”€â”€ clean.csv              # Cleaned data (CSV)
â”‚   â”œâ”€â”€ loaded_data.csv        # Loaded data (CSV)
â”‚   â”œâ”€â”€ loaded_data.json       # Loaded data (JSON)
â”‚   â””â”€â”€ requÃ¨tes.sql           # Sample SQL queries
â”‚
â””â”€â”€ ğŸ“ logs/                   # Log files
    â””â”€â”€ app.log                # Pipeline logs
```

---

## ğŸ”„ ETL Pipeline

### **1ï¸âƒ£ EXTRACT Phase - Data Extraction**

**File:** [etl/Extract.py](etl/Extract.py)

ğŸ“¡ **Fetches weather data** from OpenWeatherMap API for 5 cities:
- ğŸŒ Ouagadougou (Burkina Faso)
- ğŸŒƒ New York (USA)
- ğŸ´ó §ó ¢ó ¥ó ®ó §ó ¿ London (UK)
- ğŸ—¾ Tokyo (Japan)
- ğŸ¦˜ Sydney (Australia)

**Extracted data:**
```json
{
  "name": "Ouagadougou",
  "sys": {"country": "BF"},
  "main": {
    "temp": 300.22,
    "temp_min": 300.22,
    "temp_max": 300.22,
    "humidity": 17
  },
  "weather": [{"description": "clear sky"}],
  "wind": {"speed": 2.57}
}
```

**Actions:**
- âœ… API call with error handling
- âœ… Save to JSON files
- âœ… Detailed logging of each step

---

### **2ï¸âƒ£ TRANSFORM Phase - Data Transformation**

**File:** [etl/Transform.py](etl/Transform.py)

ğŸ§¹ **Cleans and normalizes** raw data:

**Process:**
1. **Load** JSON files from `/data`
2. **Extract** relevant fields
3. **Remove duplicates** with `drop_duplicates()`
4. **Handle missing values** with `dropna()`
5. **Convert types** to numeric
6. **Add timestamp** (scrape date)
7. **Save** to CSV for verification

**Transformed data:**
```
     city     country   temp  temp_min  temp_max  humidity           description  wind_speed         scrape_date
0  Ouagadougou   BF  300.22    300.22    300.22        17           clear sky           2.57  2026-02-04 23:32:15
1    New York   US  273.53    271.46    274.13        36           clear sky           4.12  2026-02-04 23:32:15
2     London   GB  281.19    280.32    281.82        84      broken clouds           8.23  2026-02-04 23:32:15
3      Tokyo   JP  278.48    276.82    279.94        56        few clouds           2.57  2026-02-04 23:32:15
4     Sydney   AU  301.61    300.76    303.12        58           clear sky           4.12  2026-02-04 23:32:15
```

**Applied improvements:**
- âœ… Remove rows with missing temperature or humidity
- âœ… Normalize data types
- âœ… Add universal timestamp
- âœ… Data validation

---

### **3ï¸âƒ£ LOAD Phase - Data Loading**

**File:** [etl/Load.py](etl/Load.py)

ğŸ’¾ **Loads transformed data** into MySQL database

**Database architecture:**
```sql
CREATE TABLE weather_data (
  id INT PRIMARY KEY AUTO_INCREMENT,
  ville VARCHAR(50) NOT NULL,
  pays VARCHAR(5) NOT NULL,
  temp FLOAT,
  temp_min FLOAT,
  temp_max FLOAT,
  humidite INT,
  description VARCHAR(100),
  vitesse_vent FLOAT,
  scrape_date DATETIME NOT NULL
);
```

**Actions:**
- âœ… Create table if it doesn't exist
- âœ… Insert data with `APPEND` mode
- âœ… Backup save to CSV and JSON
- âœ… Handle connection errors

**Fallback:** If MySQL is unavailable, data is saved locally in CSV/JSON

---

## ğŸ› ï¸ Configuration

### ğŸ“ Logger Configuration

**File:** [config/logger.py](config/logger.py)

```python
import logging

def setup_logger():
    logger = logging.getLogger("etl_pipeline")
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)
    return logger
```

### ğŸ“‹ Dependencies

**File:** [requirements.txt](requirements.txt)

```
pandas==3.0.0
numpy==2.4.2
requests==2.32.5
sqlalchemy==2.0.46
pymysql==1.1.2
beautifulsoup4==4.14.3
pyyaml==6.0.3
```

Installation:
```bash
pip install -r requirements.txt
```

---

## ğŸ“Š Data

### ğŸ—‚ï¸ Data Files

| File | Format | Description |
|---------|--------|-------------|
| `data/*.json` | JSON | Raw API data |
| `data_clean/clean.csv` | CSV | Cleaned data |
| `data_clean/loaded_data.csv` | CSV | Data loaded to DB |
| `data_clean/loaded_data.json` | JSON Lines | Data loaded as JSON |

### ğŸ“ˆ Expected Statistics

- **Extracted cities:** 5
- **Fields per city:** 9 (city, country, temp, temp_min, temp_max, humidity, description, wind_speed, scrape_date)
- **Total rows:** 5 (one per city)
- **Data format:** Kelvin (API) â†’ Converted for storage

---

## ğŸ› Troubleshooting

### âŒ Error: `ModuleNotFoundError: No module named 'pymysql'`

**Solution:**
```bash
pip install pymysql
```

### âŒ Error: `Connection refused` for MySQL

**Check the MySQL service:**
```bash
# Status
sudo systemctl status mysql

# Start
sudo systemctl start mysql
```

### âŒ Error: `Access denied for user 'davy'`

**Check credentials:**
1. Open [etl/Load.py](etl/Load.py)
2. Verify the connection string
3. Ensure the MySQL user exists:
```sql
CREATE USER 'davy'@'localhost' IDENTIFIED BY 'password123';
GRANT ALL PRIVILEGES ON First_Data.* TO 'davy'@'localhost';
FLUSH PRIVILEGES;
```

### âŒ Error: `Request timeout` during extraction

**Solution:**
- Check Internet connection
- Verify OpenWeatherMap API key
- Restart the pipeline

### âš ï¸ Warning: `Database driver not available`

**Meaning:** `pymysql` is not installed, data is saved locally
**Solution:** `pip install pymysql`

---

## ğŸ“Š Complete Execution Result

```
2026-02-04 23:31:25,222 - INFO - Logger configured.
[EXTRACT] Starting extraction...
2026-02-04 23:32:14,137 - INFO - Data for Ouagadougou: {...}
2026-02-04 23:32:14,450 - INFO - Data for New York: {...}
2026-02-04 23:32:14,730 - INFO - Data for Londres: {...}
2026-02-04 23:32:15,012 - INFO - Data for Tokyo: {...}
2026-02-04 23:32:15,314 - INFO - Data for Sydney: {...}
2026-02-04 23:32:15,314 - INFO - [TRANSFORM] Starting transformation
2026-02-04 23:32:15,324 - INFO - [TRANSFORM] DataFrame created with shape (5, 8)
2026-02-04 23:32:15,341 - INFO - [TRANSFORM] Rows before: 5 â†’ after: 5
2026-02-04 23:32:15,357 - INFO - [TRANSFORM] Cleaned data saved to clean.csv
2026-02-04 23:32:15,662 - INFO - Data loaded successfully
```

---

## ğŸ“– Sample SQL Queries

```sql
-- Average temperature by country
SELECT country, AVG(temp) as avg_temp
FROM weather_data
GROUP BY country
ORDER BY avg_temp DESC;

-- Most humid cities
SELECT city, humidity
FROM weather_data
ORDER BY humidity DESC
LIMIT 3;

-- Latest data
SELECT * FROM weather_data
ORDER BY scrape_date DESC
LIMIT 10;
```

---

## ğŸ¤ Contributing

Contributions are welcome! To propose an improvement:

1. ğŸ´ Fork the project
2. ğŸŒ¿ Create a branch (`git checkout -b feature/AmazingFeature`)
3. ğŸ“ Commit your changes (`git commit -m 'Add AmazingFeature'`)
4. ğŸ“¤ Push to the branch (`git push origin feature/AmazingFeature`)
5. ğŸ”„ Open a Pull Request

---

## ğŸ‘¤ Author

**Davy** - [GitHub](https://github.com/VyDonald) | [Email](mailto:tuwendedavy226@gmail.com)

---

## ğŸ™ Acknowledgments

- ğŸŒ [OpenWeatherMap API](https://openweathermap.org/api)
- ğŸ [Pandas Documentation](https://pandas.pydata.org/)
- ğŸ—„ï¸ [SQLAlchemy ORM](https://www.sqlalchemy.org/)

---

## ğŸ“ Support

For any questions or issues, please:
- ğŸ“ Open an **Issue** on GitHub
- ğŸ’¬ Contact me directly

---

<div align="center">

### â­ If this project helped you, feel free to give it a star!

**Last updated:** February 4, 2026

</div>
